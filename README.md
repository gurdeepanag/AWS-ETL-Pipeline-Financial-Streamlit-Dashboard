# Live Data Pipeline for Financial Data Modeling

## Section 1: Problem Statement and a Concise Summary of Results

Problem Statement: How can a robust data pipeline infrastructure support value stock investing by leveraging real-time financial data updates for individuals with limited financial knowledge?

This project successfully implemented a live data pipeline for financial data modeling, aimed at supporting value stock investing for individuals with limited financial knowledge. Leveraging AWS cloud infrastructure (Lambda, S3, RDS, ECS, Fargate), the pipeline automated data collection, cleaning, transformation, storage, and serving processes. Financial data was scraped from Yahoo Finance using the "yfinance" Python module, ensuring regular updates and accuracy. The resulting Streamlit dashboard, provided users with dynamic and interactive visualizations, facilitating insights into various companies' stock performance. Despite limitations such as expertise gaps and Lambda constraints, the project laid a strong foundation for future enhancements, including deeper visualizations, exploring AWS QuickSight for comprehensive dashboards, and leveraging web scraping for dynamic data acquisition. Overall, the project demonstrated the effective implementation of a robust data pipeline for financial data modeling on the AWS cloud platform, highlighting key learnings in AWS product usage, code optimization, and project management.

## Section 2: Data Engineering Lifecycle

### Section 2.1: Data Generation

The source for our data is the Yahoo Finance website, from which the data is scraped using the “yfinance” Python module which provides us with a range of information in regards with the various characteristics of the company you provide the name of such as its general information, financial statements, stock price, etc.
The manner in which the module works is that by providing the ticker code for the company, it returns an object with all the information scraped from the internet and each piece of this information is present as an attribute within this object in the form of a Pandas DataFrame. The rationale behind opting for this particular type of data stems from its capacity to ensure the continual upkeep of our information. Given that this form of data undergoes daily updates while the website structure would continue to remain the same over the long-run, it afforded us the advantage of maintaining a consistently refreshed dataset that is able to scrape data accurately on regular basis, thereby enhancing the accuracy and relevance of our information over time.

### Section 2.2: Data Ingestion

We have a series of Python Scripts that are responsible for the three steps on a general ETL process. The first file, retrieval.py is scheduled to run on a regular basis that pulls all the data using the above mentioned module. The schedule depends on how frequent the data refreshes such as stock price change daily while financial statements change quarterly. We were forced to use an EC2 instance, as opposed to a Lambda/Eventbridge instance, as there is a 15 minute timeout by default for Lambda, due to our extremely large dataset, this was not feasible, and thus we had to make use of an EC2 instance instead. This file simply extracts all the information for a static list of tickers we have identified and places them in the form of Parquet files on S3 as demonstrated in the Pipeline Diagram. This is also an area where we faced huge performance issues as the EC2 instance wasn’t capable of executing for all the tickers at once. Therefore, we implemented batching without logic to divide the list of tickers into smaller chunks, generate these files with just the information about the tickers in one chunk, place them on S3 and move to the next chunk. While without batching the code didn’t execute to the end, with it incorporated, the entire extraction process took ~30 minutes.

On its completion, this script also updates a file called RetrievalCompleted.txt. Subsequently, this triggers an AWS Lambda function which runs a Python script cleaning and organizing them into another set of Parquet files divided based on the information they carry, which are then also stored on S3. Similar to our previous script, this script updates a file called CleaningCompleted.txt which triggers our final Lambda function for our ETL process to be completed that runs a python script pushing all the clean transformed data into an AWS Postgresql RDS. The reason for the use of Parquet data format is the data compression that comes with it to not only reduce the memory consumption on our free-tier EC2 instance but also incur significantly lesser S3 costs.

### Section 2.3: Data Storage

Our data storage strategy leverages the strengths of two AWS services to optimize cost and performance. Amazon S3 serves as our cost-effective workhorse for storing large amounts of raw and intermediate data in CSV/Parquet format. Its scalability ensures we can accommodate growing data volumes, while its durability safeguards our information. Furthermore, S3 acts as a convenient staging ground for processing, as Lambda functions can be triggered by new data uploads to initiate automated cleaning and organization. Once the data is cleaned and transformed, it migrates to Amazon RDS Postgresql. This managed relational database service is ideal for storing our final, usable data due to its efficient querying capabilities via familiar SQL language. This is crucial as users will be querying the data on-demand through the dashboard, requiring fast and structured access. Additionally, RDS Postgresql offers robust data integrity features and advanced security options to safeguard sensitive financial information. By segregating storage based on data purpose and access needs, this approach provides a cost-effective, secure, and performant solution for our stock price dashboard project.

### Section 2.4: Data Transformation

The data obtained from “yfinance” while being clean for the most part, we did have to make a few changes for analytical or cost-effective purposes. To ensure seamless functionality and integrity, the first step was to ensure we kept just the relevant columns and provided appropriate names for each column adhering to the SQL standards. While the data was relatively free from missing values, there were instances by virtue of actual missing data on the Yahoo Finance Website which we backfilled using some domain expertise.
Besides the data already formatted in a neat tabular format, features and columns which were directly not present without our datasets were created. The aim of doing this was to reduce the computation time when refreshing our dashboards as the user inputs the ticker names, the script would just extract the information already in a tabular format and plot it in near real-time. Compressing the data in Parquet format for cost and performance effectiveness, the transformed data gets transferred to S3, alleviating memory constraints and allowing smooth processing.

### Section 2.5: Data Serving

In our approach to data serving, we chose to present our results through a Streamlit dashboard hosted on AWS Fargate using Elastic Container Service (ECS). We selected Fargate for its ability to dynamically adjust to user demand, anticipating varying levels of traffic to our dashboard. To achieve this, we built a Docker image with a foundation of Amazon Linux, incorporating our frontend code and GIF, along with essential tools such as Python, pip, Streamlit, SQLAlchemy, Pandas, Plotly, and psycopg2. This Docker image was uploaded to Docker Hub and deployed on ECS for reliable and consistent execution, ensuring all necessary packages, dependencies, and files are readily available. In our decision-making process, we prioritized Docker to guarantee the reliability and consistency of our code execution, ensuring all essential components are consistently accessible. For data querying, we opted for SQL to get the data needed from the Amazon RDS stuffed into a Pandas DataFrame using SQLAlchemy. As DataFrames allow us to make the plots quickly while still providing the quick execution that SQL enables us with, it was the way to go for us. Choosing Streamlit for our dashboard was driven by its flexibility, customization options, and seamless integration with Python and Plotly for creating dynamic and interactive visualizations. To enhance user experience, we incorporated a sidebar to separate text input, industry, and company descriptions to separate it from the core financial information/data. We also ensured that our text input box is not case-sensitive and that we included core financial definitions and information on the dashboard for improved usability for beginners. Moreover, we added a motivation button, featuring a GIF, to inspire users and foster a productive mindset. This comprehensive approach ensures our dashboard meets user needs effectively while providing a seamless and engaging experience.

## Section 3: Limitations and Possible Next Steps

The approach we adopted for this dashboard was to cover a few basic visualizations that would enable people new to trading to get a feel of the different KPIs they must take a look at to make their decision. Partly due to lack of sufficient expertise in this domain, one key limitation is that our dashboard doesn’t delve into the deeper aspects that might make the person confident about their trading decision. While the script does extract all the possible information about the company, we are using approximately 15%-20% of the available data for our visualizations. Getting someone with more domain expertise and collaborating with them to design visualizations catered towards the new traders and creating an alternative dashboard using a toggle for experts would increase the usability of our dashboard since the data is already available. Secondly, we could build a completely AWS-Native Dashboard solution by leveraging AWS QuickSight rather than use an EC2 instance and Streamlit for creating the dashboard. This would have enabled us to cover a lot more visualizations as the AWS offering provides a GUI interface similar to Power BI/Tableau to work on an existing data model, which is already built in AWS RDS in our case. Another limitation we faced was the 15 minute timeout for AWS Lambda, and how it forced us to use an EC2 instance instead of Lambda/Eventbridge, a possible remedy to this problem would be to use a uninhibited AWS account with more credits and compute resources that we could assign to it, as opposed to the learner lab environment. Lastly, one technical limitation is our static list of tickers which is constrained possibly from the “yfinance” module and a lack of a function within it to provide a list of tickers. The best solution would be to move away from this list and use a web-scraping solution for obtaining the list of tickers from the internet.


## Section 4: Conclusions

Through this project, we've gained invaluable insights into building an end-to-end data pipeline on AWS cloud infrastructure. We utilized the "yfinance" Python module for continuous data scraping from Yahoo Finance, ensuring our dataset remained accurate and up-to-date. Challenges in data ingestion were overcome by implementing batching and leveraging an EC2 instance due to Lambda timeout constraints. Storage optimization was achieved by segregating data storage between Amazon S3 for raw and intermediate data and Amazon RDS Postgresql for final, usable data, ensuring efficient querying and data integrity. Efficient data transformation processes involved streamlining data, filling missing values, and compressing data in Parquet format for performance and cost-effectiveness. Hosting our dashboard on AWS Fargate using ECS, coupled with Docker for reliability, SQL querying for data retrieval, and Streamlit for dynamic visualizations, ensured seamless data serving and user interaction. Acknowledging limitations such as lack of expertise and AWS Lambda constraints, future steps include deeper visualizations with domain experts, exploring AWS QuickSight for a comprehensive dashboard solution, and utilizing web scraping for obtaining a dynamic list of tickers. Overall, this project provided us with valuable experience in AWS product usage, code optimization for cost reduction, and effective data engineering project management.

## References

yfinance. (2024) yfinance pypi. https://pypi.org/project/yfinance/
